func(blah, coms, suffixes)
coms <- c("aids", "acute_mi", "history_mi")
suffixes <- c("_99", "_00", "_01")
blah<-as.data.frame(1:100)
#You will need to include a vector of the base variable names and a vector of the suffixes IN ASCENDING ORDER
func <- function(datset, vars, suffs){
dat <- as.data.frame(datset)
for(i in vars){
rep1 <- rep(i, length(suffs))
print(rep1)
}
}
func(blah, coms, suffixes)
coms <- c("aids", "acute_mi", "history_mi")
suffixes <- c("_99", "_00", "_01")
blah<-as.data.frame(1:100)
#You will need to include a vector of the base variable names and a vector of the suffixes IN ASCENDING ORDER
func <- function(datset, vars, suffs){
dat <- as.data.frame(datset)
for(i in vars){
rep1 <- rep(i, length(suffs))
rep2 <- suffs
varlist <- paste0(rep1, rep2)
print(varlist)
}
}
func(blah, coms, suffixes)
install.packages("maps")
install.packages("ggmap")
install.packages("mapdata")
#Zipcode Map US
library(ggplot2)
library(ggmap)
library(maps)
library(mapdata)
?map_data
US <- map_data("usa")
View(US)
View(US)
View(US)
ggplot(data=US) +geom_polygon(aes(x=long, y=lat))
ggplot(data=US) +geom_polygon(aes(x=long, y=lat, group=region))
ggplot(data=US) +geom_polygon(aes(x=long, y=lat, group=group, fill=region))
View(US)
US <- map_data("counties")
US <- map_data("county")
View(US)
ggplot(data=US) +geom_polygon(aes(x=long, y=lat, group=group, fill=region))
ggplot(data=US) +geom_polygon(aes(x=long, y=lat, group=group, fill=region)) + theme(legend.position = NULL)
ggplot(data=US) +geom_polygon(aes(x=long, y=lat, group=group, fill=region)) + guides(fill = FALSE)
ggplot(data=US) + geom_polygon(aes(x=long, y=lat, group=group, fill=subregion)) + guides(fill = FALSE)
library(dplyr)
countdata <- US %>% group_by(region) %>% summarise(total = n())
View(countdata)
View(US)
View(US)
countdata <- US %>% group_by(region, subregion) %>% summarise(total = n())
View(countdata)
countdata <- US %>% group_by(region) %>% count(distinct(subregion))
countdata <- US %>% group_by(region) %>% count(unique(subregion))
countdata <- US %>% group_by(region) %>% count(subregion)
View(countdata)
countdata <- US %>% group_by(region, subregion) %>% distinct()
countdata <- US %>% select(region, subregion) %>% distinct()
View(countdata)
countdata <- US %>% select(region, subregion) %>% distinct() %>% group_by(region) %>% count()
View(countdata)
View(countdata)
merged <- left_join(US, countdata)
View(merged)
ggplot(data=US) + geom_polygon(aes(x=long, y=lat, group=group, fill=n)) + guides(fill = FALSE)
ggplot(data=US) + geom_polygon(aes(x=long, y=lat, group=group)) + guides(fill = FALSE)
View(merged)
ggplot(data=US) + geom_polygon(aes(x=long, y=lat, group=group)) + scale_fill_continuous(n)
ggplot(data=US) + geom_polygon(aes(x=long, y=lat, group=group)) + scale_fill_continuous(subregion)
ggplot(data=merged) + geom_polygon(aes(x=long, y=lat, group=group, fill=n))
US <- map_data("county")
countdata <- US %>% select(region, subregion) %>% distinct() %>% group_by(region)
merged <- left_join(US, countdata)
ggplot(data=merged) + geom_polygon(aes(x=long, y=lat, group=group, fill=n))
ggplot(data=merged) + geom_polygon(aes(x=long, y=lat, group=group, fill=n))
View(merged)
merged <- left_join(US, countdata)
US <- map_data("county")
countdata <- US %>% select(region, subregion) %>% distinct() %>% group_by(region)
countdata <- US %>% select(region, subregion) %>% distinct() %>% group_by(region) %>% count()
merged <- left_join(US, countdata)
View(merged)
ggplot(data=merged) + geom_polygon(aes(x=long, y=lat, group=group, fill=n))
View(merged)
ggplot(data=merged) + geom_polygon(aes(x=long, y=lat, group=group, fill=n))
countdata <- US %>% select(region, subregion) %>% distinct() %>% group_by(region) %>% count()
merged <- left_join(US, countdata)
ggplot(data=merged) + geom_polygon(aes(x=long, y=lat, group=group, fill=n))
?scale_fill_continuous
ggplot(data=merged) + geom_polygon(aes(x=long, y=lat, group=group, fill=n)) + scale_fill_continuous(low="blue", high="red")
area= read.csv(file="Documents\area.csv")
area= read.csv(file="Documents/area.csv")
area= read.csv(file="C:\Users\matth\Documents\area.csv")
area= read.csv(file="C:/Users/matth/Documents/area.csv")
View(area)
area= read.csv(file="C:/Users/matth/Documents/area.csv") %>% mutate(X= tolower(X))
View(area)
merged2<- left_join(merged, area, by="region")
View(merged)
area= read.csv(file="C:/Users/matth/Documents/area.csv") %>% mutate(X= tolower(X), region=X)
merged2<- left_join(merged, area, by="region")
View(merged2)
View(area)
View(merged)
library(dplyr)
library(ggplot2)
library(ggmap)
library(maps)
library(mapdata)
US <- map_data("county")
countdata <- US %>% select(region, subregion) %>% distinct() %>% group_by(region) %>% count()
merged <- left_join(US, countdata)
area= read.csv(file="C:/Users/matth/Documents/area.csv") %>% mutate(X= tolower(X), region=X)
merged2<- left_join(merged, area, by="region")
View(merged2)
View(area)
merged2<- left_join(merged, area)
View(merged2)
merged2<- left_join(US, area)
View(US)
merged2<- left_join(US, area)
area= read.csv(file="C:/Users/matth/Documents/area.csv") %>% mutate(X= tolower(X), region=X) %>% select(region, sq.mi)
View(area)
library(dplyr)
library(ggplot2)
library(ggmap)
library(maps)
library(mapdata)
US <- map_data("county")
countycount <- US %>% select(region, subregion) %>% distinct() %>% group_by(region) %>% count()
area= read.csv(file="C:/Users/matth/Documents/area.csv") %>% mutate(X= tolower(X), region=X) %>% select(region, sq.mi)
merged<- left_join(US, area)
View(merged)
View(US)
merged<- merge(US, area)
View(merged)
area$region %in% US$region
area= read.csv(file="C:/Users/matth/Documents/area.csv") %>% mutate(region= stringr::str_trim(tolower(X))) %>% select(region, sq.mi)
area$region %in% US$region
merged <- plry::join_all(US, countycount, area, by="region", type="left")
install.packages(plyr)
install.packages("plyr")
merged <- join_all(US, countycount, area, by="region", type="left")
merged <- plyr::join_all(US, countycount, area, by="region", type="left")
merged <- plyr::join_all(US, countycount, area, by="region", type="left")
merged <- plyr::join_all(list(US, countycount, area), by="region", type="left")
View(merged)
merged <- plyr::join_all(list(US, countycount, area), by="region", type="left") %>% mutate(prop=n/sq.mi)
View(merged)
merged <- plyr::join_all(list(US, countycount, area), by="region", type="left") %>% mutate(prop=sq.mi/n)
merged <- plyr::join_all(list(US, countycount, area), by="region", type="left") %>% mutate(prop=sq.mi/n)
View(merged)
ggplot(data=merged) + geom_polygon(aes(x=long, y=lat, group=group, fill=prop)) + scale_fill_continuous(low="blue", high="red")
ggplot(data=merged) + geom_polygon(aes(x=long, y=lat, group=group, fill=prop, color=region)) + scale_fill_continuous(low="blue", high="red")
ggplot(data=merged) + geom_polygon(aes(x=long, y=lat, group=group, fill=prop, color=region)) + scale_fill_continuous(low="blue", high="red") + guides(fill=FALSE, color=FALSE)
ggplot(data=merged) + geom_polygon(aes(x=long, y=lat, group=group, fill=prop, color="black")) + scale_fill_continuous(low="blue", high="red") + guides(fill=FALSE, color=FALSE)
ggplot(data=merged) + geom_polygon(aes(x=long, y=lat, group=group, fill=prop, color="black")) + scale_fill_continuous(low="blue", high="yellow") + guides(fill=FALSE, color=FALSE)
ggplot(data=merged) + geom_polygon(aes(x=long, y=lat, group=group, fill=prop, color="black")) + scale_fill_continuous(low="green", high="yellow") + guides(fill=FALSE, color=FALSE)
ggplot(data=merged) + geom_polygon(aes(x=long, y=lat, group=group, fill=prop, color="black")) + scale_fill_continuous(low="blue", high="yellow") + guides(fill=FALSE, color=FALSE)
ggplot(data=merged) + geom_polygon(aes(x=long, y=lat, group=group, fill=prop, colour="black")) + scale_fill_continuous(low="blue", high="red") + guides(fill=FALSE, color=FALSE)
ggplot(data=merged) + geom_polygon(aes(x=long, y=lat, group=group, fill=prop, colour="white")) + scale_fill_continuous(low="blue", high="red") + guides(fill=FALSE, color=FALSE)
ggplot(data=merged) + geom_polygon(aes(x=long, y=lat, group=group, fill=prop),  colour="white") + scale_fill_continuous(low="blue", high="red") + guides(fill=FALSE, color=FALSE)
ggplot(data=merged) + geom_polygon(aes(x=long, y=lat, group=group, fill=prop),  colour="white") + scale_fill_continuous(low="white", high="black") + guides(fill=FALSE, color=FALSE)
ggplot(data=merged) + geom_polygon(aes(x=long, y=lat, group=group, fill=prop),  colour="yellow") + scale_fill_continuous(low="white", high="black") + guides(fill=FALSE, color=FALSE)
################################################################################
#Updated the data for new analyses                                             #
################################################################################
#LDA analysis in R
setwd("C:/Users/matth/Documents/GitHub/QS2020/QuitSmokingTweets")
library(tidyverse)
library(lubridate)
library(gridExtra)
library(lsmeans)
dat2018<- read.csv("C:/Users/matth/Documents/GitHub/QS2020/QuitSmokingTweets/processedWide2018.csv")
dat2019<- read.csv("C:/Users/matth/Documents/GitHub/QS2020/QuitSmokingTweets/processedWide2019.csv")
dat2020<- read.csv("C:/Users/matth/Documents/GitHub/QS2020/QuitSmokingTweets/processedWide2020.csv")
datNEW<- read.csv("C:/Users/matth/Documents/GitHub/QS2020/QuitSmokingTweets/processedWideNEW.csv")
wideData<- rbind(dat2018, dat2019, dat2020, datNEW) %>% mutate(newDate=as.Date(PUBLISH_DATE, "%m/%d/%Y")) %>%
mutate(tweetYear= year(newDate), tweetMonth= month(newDate), tweetDay=day(newDate)) %>%
group_by(tweetYear) %>% add_count(tweetYear, name="totalYearlyTweets") %>%
group_by(tweetMonth) %>% add_count(tweetMonth, name= "totalMonthlyTweets") %>%
ungroup() %>%
mutate(tweetWeek= week(newDate)) %>% add_count(tweetWeek, name= "totalWeeklyTweets") %>% #isoweek instead of week is probably needed for Trends data
distinct(ARTICLE_URL, .keep_all = TRUE) #when we brought in the new data there was overlap but we didnt remove it in the code because we needed identical dictionaries
#Separate out the topics and percentages, along with the date for graphing and the unique ID variable (ARTICLE_URL)
A <- wideData %>% select(ARTICLE_URL, CONTENT, newDate, tweetYear, tweetMonth, tweetWeek, zero_top, zero_perc) %>% rename(Topic = zero_top, Probability = zero_perc)
B <- wideData %>% select(ARTICLE_URL, CONTENT, newDate, tweetYear, tweetMonth, tweetWeek, one_top, one_perc) %>% filter(is.na(one_top)==FALSE) %>% rename(Topic = one_top, Probability = one_perc)
C <- wideData %>% select(ARTICLE_URL, CONTENT, newDate, tweetYear, tweetMonth, tweetWeek, two_top, two_perc) %>% filter(is.na(two_top)==FALSE) %>% rename(Topic = two_top, Probability = two_perc)
D <- wideData %>% select(ARTICLE_URL, CONTENT, newDate, tweetYear, tweetMonth, tweetWeek, three_top, three_perc) %>% filter(is.na(three_top)==FALSE) %>% rename(Topic = three_top, Probability = three_perc)
E <- wideData %>% select(ARTICLE_URL, CONTENT, newDate, tweetYear, tweetMonth, tweetWeek, four_top, four_perc) %>% filter(is.na(four_top)==FALSE) %>% rename(Topic = four_top, Probability = four_perc)
G <- wideData %>% select(ARTICLE_URL, CONTENT, newDate, tweetYear, tweetMonth, tweetWeek, five_top, five_perc) %>% filter(is.na(five_top)==FALSE) %>% rename(Topic = five_top, Probability = five_perc)
H <- wideData %>% select(ARTICLE_URL, CONTENT, newDate, tweetYear, tweetMonth, tweetWeek, six_top, six_perc) %>% filter(is.na(six_top)==FALSE) %>% rename(Topic = six_top, Probability = six_perc)
I <- wideData %>% select(ARTICLE_URL, CONTENT, newDate, tweetYear, tweetMonth, tweetWeek, seven_top, seven_perc) %>% filter(is.na(seven_top)==FALSE) %>% rename(Topic = seven_top, Probability = seven_perc)
J <- wideData %>% select(ARTICLE_URL, CONTENT, newDate, tweetYear, tweetMonth, tweetWeek, eight_top, eight_perc) %>% filter(is.na(eight_top)==FALSE) %>% rename(Topic = eight_top, Probability = eight_perc)
K <- wideData %>% select(ARTICLE_URL, CONTENT, newDate, tweetYear, tweetMonth, tweetWeek, nine_top, nine_perc) %>% filter(is.na(nine_top)==FALSE) %>% rename(Topic = nine_top, Probability = nine_perc)
longData<- rbind(A,B,C,D,E,G,H,I,J,K)
#add coronavirus mentions
longData$corona <- grepl("coronavirus", longData$CONTENT, ignore.case = TRUE)
longData$covid <- grepl("covid", longData$CONTENT, ignore.case = TRUE)
longData$hasCovid <- ifelse(longData$corona == TRUE | longData$covid == TRUE, 1, 0)
###########################################################################################################
#Lee wanted yearly average daily tweets and standard deviations
leeData<- longData %>% mutate(Topic = case_when(Topic == 0 ~ "Health Ch",
Topic == 1 ~ "Need 2 Qt",
Topic == 4 ~ "E-Cig",
Topic == 6 ~ "Clinics/Serv",
Topic == 7 ~ "Advice/Suc",
Topic == 8 ~ "Personal Ex")) %>%
filter(is.na(Topic)==FALSE) %>% group_by(tweetYear) %>% count(newDate, wt=Probability) %>%summarise(mean=mean(n), sd=sd(n))
monthlySummedLong<- longData %>% group_by(tweetYear,tweetMonth) %>% count(Topic, wt=Probability) %>%
mutate(Topic = case_when(Topic == 0 ~ "Health Ch",
Topic == 1 ~ "Need 2 Qt",
Topic == 4 ~ "E-Cig",
Topic == 6 ~ "Clinics/Serv",
Topic == 7 ~ "Advice/Suc",
Topic == 8 ~ "Personal Ex")) %>% #note: other topics are NA unless we assign them something
mutate(tweetMonth = as.factor(tweetMonth)) %>%
ungroup() %>%
mutate(tweetMonth = as.factor(tweetMonth)) %>%
mutate(Quarter = case_when(tweetMonth %in% c(1,2,3) ~ "Q1",
tweetMonth %in% c(4,5,6) ~ "Q2",
tweetMonth %in% c(7,8,9) ~ "Q3",
tweetMonth %in% c(10,11,12) ~ "Q4"))
CVmonthlySummedLong<- longData %>% filter(hasCovid==1) %>% group_by(tweetYear,tweetMonth) %>% count(Topic, wt=Probability) %>%
mutate(Topic = case_when(Topic == 0 ~ "Health Ch",
Topic == 1 ~ "Need 2 Qt",
Topic == 4 ~ "E-Cig",
Topic == 6 ~ "Clinics/Serv",
Topic == 7 ~ "Advice/Suc",
Topic == 8 ~ "Personal Ex")) %>% #note: other topics are NA unless we assign them something
mutate(tweetMonth = as.factor(tweetMonth)) %>%
ungroup() %>%
mutate(tweetMonth = as.factor(tweetMonth)) %>%
mutate(Quarter = case_when(tweetMonth %in% c(1,2,3) ~ "Q1",
tweetMonth %in% c(4,5,6) ~ "Q2",
tweetMonth %in% c(7,8,9) ~ "Q3",
tweetMonth %in% c(10,11,12) ~ "Q4"))
#######################################################################################################################
#For most of the tables
#######################################################################################################################
dailySummedLong<- longData %>% group_by(tweetYear,tweetMonth,newDate) %>% count(Topic, wt=Probability) %>%
mutate(Topic = case_when(Topic == 0 ~ "Health Ch",
Topic == 1 ~ "Need 2 Qt",
Topic == 4 ~ "E-Cig",
Topic == 6 ~ "Clinics/Serv",
Topic == 7 ~ "Advice/Suc",
Topic == 8 ~ "Personal Ex")) %>% #note: other topics are NA unless we assign them something
ungroup() %>%
mutate(tweetMonth = as.factor(tweetMonth)) %>%
mutate(Quarter = case_when(tweetMonth %in% c(1,2,3) ~ "Q1",
tweetMonth %in% c(4,5,6) ~ "Q2",
tweetMonth %in% c(7,8,9) ~ "Q3",
tweetMonth %in% c(10,11,12) ~ "Q4"))
#for coronavirus table
QuarterlyNoSpamTotal2020<- monthlySummedLong %>% filter(is.na(Topic)==FALSE, tweetYear==2020) %>%
group_by(tweetYear) %>% count(Quarter, wt=n)
QuarterlyNoSpamTotalTopics2020<- monthlySummedLong %>% filter(is.na(Topic)==FALSE, tweetYear==2020) %>%
group_by(tweetYear,Topic) %>% count(Quarter, wt=n)
#same as above but only corona
QuarterlyNoSpamTotal2020CV<- CVmonthlySummedLong %>% filter(is.na(Topic)==FALSE, tweetYear==2020) %>%
group_by(tweetYear) %>% count(Quarter, wt=n)
QuarterlyNoSpamTotalTopics2020CV<- CVmonthlySummedLong %>% filter(is.na(Topic)==FALSE, tweetYear==2020) %>%
group_by(tweetYear,Topic) %>% count(Quarter, wt=n)
################################################
dailyNoSpamSummedLong<- dailySummedLong %>% filter(is.na(Topic)==FALSE)
###########################################################################################################
#Create tables with daily mean topic tweets per quarter                                                   #
###########################################################################################################
DailyAllTopic <- dailyNoSpamSummedLong %>% group_by(tweetYear, Quarter) %>% count(newDate, wt=n)
Q1 <- DailyAllTopic %>% filter(Quarter=="Q1") %>% mutate(tweetYear = as.factor(tweetYear))
Q2 <- DailyAllTopic %>% filter(Quarter=="Q2") %>% mutate(tweetYear = as.factor(tweetYear))
Q3 <- DailyAllTopic %>% filter(Quarter=="Q3") %>% mutate(tweetYear = as.factor(tweetYear))
Q4 <- DailyAllTopic %>% filter(Quarter=="Q4") %>% mutate(tweetYear = as.factor(tweetYear))
aovQ1 <- aov(n ~ tweetYear, data = Q1)
summary(aovQ1)
model.tables(aovQ1, "means")
summary.lm(aovQ1)
################################################################################
#Updated the data for new analyses                                             #
################################################################################
#LDA analysis in R
setwd("C:/Users/matth/Documents/GitHub/QS2020/QuitSmokingTweets")
library(tidyverse)
library(lubridate)
library(gridExtra)
library(lsmeans)
dat2018<- read.csv("C:/Users/matth/Documents/GitHub/QS2020/QuitSmokingTweets/processedWide2018.csv")
dat2019<- read.csv("C:/Users/matth/Documents/GitHub/QS2020/QuitSmokingTweets/processedWide2019.csv")
dat2020<- read.csv("C:/Users/matth/Documents/GitHub/QS2020/QuitSmokingTweets/processedWide2020.csv")
datNEW<- read.csv("C:/Users/matth/Documents/GitHub/QS2020/QuitSmokingTweets/processedWideNEW.csv")
wideData<- rbind(dat2018, dat2019, dat2020, datNEW) %>% mutate(newDate=as.Date(PUBLISH_DATE, "%m/%d/%Y")) %>%
mutate(tweetYear= year(newDate), tweetMonth= month(newDate), tweetDay=day(newDate)) %>%
group_by(tweetYear) %>% add_count(tweetYear, name="totalYearlyTweets") %>%
group_by(tweetMonth) %>% add_count(tweetMonth, name= "totalMonthlyTweets") %>%
ungroup() %>%
mutate(tweetWeek= week(newDate)) %>% add_count(tweetWeek, name= "totalWeeklyTweets") %>% #isoweek instead of week is probably needed for Trends data
distinct(ARTICLE_URL, .keep_all = TRUE) #when we brought in the new data there was overlap but we didnt remove it in the code because we needed identical dictionaries
#Separate out the topics and percentages, along with the date for graphing and the unique ID variable (ARTICLE_URL)
A <- wideData %>% select(ARTICLE_URL, CONTENT, newDate, tweetYear, tweetMonth, tweetWeek, zero_top, zero_perc) %>% rename(Topic = zero_top, Probability = zero_perc)
B <- wideData %>% select(ARTICLE_URL, CONTENT, newDate, tweetYear, tweetMonth, tweetWeek, one_top, one_perc) %>% filter(is.na(one_top)==FALSE) %>% rename(Topic = one_top, Probability = one_perc)
C <- wideData %>% select(ARTICLE_URL, CONTENT, newDate, tweetYear, tweetMonth, tweetWeek, two_top, two_perc) %>% filter(is.na(two_top)==FALSE) %>% rename(Topic = two_top, Probability = two_perc)
D <- wideData %>% select(ARTICLE_URL, CONTENT, newDate, tweetYear, tweetMonth, tweetWeek, three_top, three_perc) %>% filter(is.na(three_top)==FALSE) %>% rename(Topic = three_top, Probability = three_perc)
E <- wideData %>% select(ARTICLE_URL, CONTENT, newDate, tweetYear, tweetMonth, tweetWeek, four_top, four_perc) %>% filter(is.na(four_top)==FALSE) %>% rename(Topic = four_top, Probability = four_perc)
G <- wideData %>% select(ARTICLE_URL, CONTENT, newDate, tweetYear, tweetMonth, tweetWeek, five_top, five_perc) %>% filter(is.na(five_top)==FALSE) %>% rename(Topic = five_top, Probability = five_perc)
H <- wideData %>% select(ARTICLE_URL, CONTENT, newDate, tweetYear, tweetMonth, tweetWeek, six_top, six_perc) %>% filter(is.na(six_top)==FALSE) %>% rename(Topic = six_top, Probability = six_perc)
I <- wideData %>% select(ARTICLE_URL, CONTENT, newDate, tweetYear, tweetMonth, tweetWeek, seven_top, seven_perc) %>% filter(is.na(seven_top)==FALSE) %>% rename(Topic = seven_top, Probability = seven_perc)
J <- wideData %>% select(ARTICLE_URL, CONTENT, newDate, tweetYear, tweetMonth, tweetWeek, eight_top, eight_perc) %>% filter(is.na(eight_top)==FALSE) %>% rename(Topic = eight_top, Probability = eight_perc)
K <- wideData %>% select(ARTICLE_URL, CONTENT, newDate, tweetYear, tweetMonth, tweetWeek, nine_top, nine_perc) %>% filter(is.na(nine_top)==FALSE) %>% rename(Topic = nine_top, Probability = nine_perc)
longData<- rbind(A,B,C,D,E,G,H,I,J,K)
#add coronavirus mentions
longData$corona <- grepl("coronavirus", longData$CONTENT, ignore.case = TRUE)
longData$covid <- grepl("covid", longData$CONTENT, ignore.case = TRUE)
longData$hasCovid <- ifelse(longData$corona == TRUE | longData$covid == TRUE, 1, 0)
###########################################################################################################
#Lee wanted yearly average daily tweets and standard deviations
leeData<- longData %>% mutate(Topic = case_when(Topic == 0 ~ "Health Ch",
Topic == 1 ~ "Need 2 Qt",
Topic == 4 ~ "E-Cig",
Topic == 6 ~ "Clinics/Serv",
Topic == 7 ~ "Advice/Suc",
Topic == 8 ~ "Personal Ex")) %>%
filter(is.na(Topic)==FALSE) %>% group_by(tweetYear) %>% count(newDate, wt=Probability) %>%summarise(mean=mean(n), sd=sd(n))
monthlySummedLong<- longData %>% group_by(tweetYear,tweetMonth) %>% count(Topic, wt=Probability) %>%
mutate(Topic = case_when(Topic == 0 ~ "Health Ch",
Topic == 1 ~ "Need 2 Qt",
Topic == 4 ~ "E-Cig",
Topic == 6 ~ "Clinics/Serv",
Topic == 7 ~ "Advice/Suc",
Topic == 8 ~ "Personal Ex")) %>% #note: other topics are NA unless we assign them something
mutate(tweetMonth = as.factor(tweetMonth)) %>%
ungroup() %>%
mutate(tweetMonth = as.factor(tweetMonth)) %>%
mutate(Quarter = case_when(tweetMonth %in% c(1,2,3) ~ "Q1",
tweetMonth %in% c(4,5,6) ~ "Q2",
tweetMonth %in% c(7,8,9) ~ "Q3",
tweetMonth %in% c(10,11,12) ~ "Q4"))
################################################################################
#Updated the data for new analyses                                             #
################################################################################
#LDA analysis in R
setwd("C:/Users/matth/Documents/GitHub/QS2020/QuitSmokingTweets")
library(tidyverse)
library(lubridate)
library(gridExtra)
library(lsmeans)
dat2018<- read.csv("C:/Users/matth/Documents/GitHub/QS2020/QuitSmokingTweets/processedWide2018.csv")
dat2019<- read.csv("C:/Users/matth/Documents/GitHub/QS2020/QuitSmokingTweets/processedWide2019.csv")
dat2020<- read.csv("C:/Users/matth/Documents/GitHub/QS2020/QuitSmokingTweets/processedWide2020.csv")
datNEW<- read.csv("C:/Users/matth/Documents/GitHub/QS2020/QuitSmokingTweets/processedWideNEW.csv")
wideData<- rbind(dat2018, dat2019, dat2020, datNEW) %>% mutate(newDate=as.Date(PUBLISH_DATE, "%m/%d/%Y")) %>%
mutate(tweetYear= year(newDate), tweetMonth= month(newDate), tweetDay=day(newDate)) %>%
group_by(tweetYear) %>% add_count(tweetYear, name="totalYearlyTweets") %>%
group_by(tweetMonth) %>% add_count(tweetMonth, name= "totalMonthlyTweets") %>%
ungroup() %>%
mutate(tweetWeek= week(newDate)) %>% add_count(tweetWeek, name= "totalWeeklyTweets") %>% #isoweek instead of week is probably needed for Trends data
distinct(ARTICLE_URL, .keep_all = TRUE) #when we brought in the new data there was overlap but we didnt remove it in the code because we needed identical dictionaries
#Separate out the topics and percentages, along with the date for graphing and the unique ID variable (ARTICLE_URL)
A <- wideData %>% select(ARTICLE_URL, CONTENT, newDate, tweetYear, tweetMonth, tweetWeek, zero_top, zero_perc) %>% rename(Topic = zero_top, Probability = zero_perc)
B <- wideData %>% select(ARTICLE_URL, CONTENT, newDate, tweetYear, tweetMonth, tweetWeek, one_top, one_perc) %>% filter(is.na(one_top)==FALSE) %>% rename(Topic = one_top, Probability = one_perc)
C <- wideData %>% select(ARTICLE_URL, CONTENT, newDate, tweetYear, tweetMonth, tweetWeek, two_top, two_perc) %>% filter(is.na(two_top)==FALSE) %>% rename(Topic = two_top, Probability = two_perc)
D <- wideData %>% select(ARTICLE_URL, CONTENT, newDate, tweetYear, tweetMonth, tweetWeek, three_top, three_perc) %>% filter(is.na(three_top)==FALSE) %>% rename(Topic = three_top, Probability = three_perc)
E <- wideData %>% select(ARTICLE_URL, CONTENT, newDate, tweetYear, tweetMonth, tweetWeek, four_top, four_perc) %>% filter(is.na(four_top)==FALSE) %>% rename(Topic = four_top, Probability = four_perc)
G <- wideData %>% select(ARTICLE_URL, CONTENT, newDate, tweetYear, tweetMonth, tweetWeek, five_top, five_perc) %>% filter(is.na(five_top)==FALSE) %>% rename(Topic = five_top, Probability = five_perc)
H <- wideData %>% select(ARTICLE_URL, CONTENT, newDate, tweetYear, tweetMonth, tweetWeek, six_top, six_perc) %>% filter(is.na(six_top)==FALSE) %>% rename(Topic = six_top, Probability = six_perc)
I <- wideData %>% select(ARTICLE_URL, CONTENT, newDate, tweetYear, tweetMonth, tweetWeek, seven_top, seven_perc) %>% filter(is.na(seven_top)==FALSE) %>% rename(Topic = seven_top, Probability = seven_perc)
J <- wideData %>% select(ARTICLE_URL, CONTENT, newDate, tweetYear, tweetMonth, tweetWeek, eight_top, eight_perc) %>% filter(is.na(eight_top)==FALSE) %>% rename(Topic = eight_top, Probability = eight_perc)
K <- wideData %>% select(ARTICLE_URL, CONTENT, newDate, tweetYear, tweetMonth, tweetWeek, nine_top, nine_perc) %>% filter(is.na(nine_top)==FALSE) %>% rename(Topic = nine_top, Probability = nine_perc)
longData<- rbind(A,B,C,D,E,G,H,I,J,K)
#add coronavirus mentions
longData$corona <- grepl("coronavirus", longData$CONTENT, ignore.case = TRUE)
longData$covid <- grepl("covid", longData$CONTENT, ignore.case = TRUE)
longData$hasCovid <- ifelse(longData$corona == TRUE | longData$covid == TRUE, 1, 0)
###########################################################################################################
#Lee wanted yearly average daily tweets and standard deviations
leeData<- longData %>% mutate(Topic = case_when(Topic == 0 ~ "Health Ch",
Topic == 1 ~ "Need 2 Qt",
Topic == 4 ~ "E-Cig",
Topic == 6 ~ "Clinics/Serv",
Topic == 7 ~ "Advice/Suc",
Topic == 8 ~ "Personal Ex")) %>%
filter(is.na(Topic)==FALSE) %>% group_by(tweetYear) %>% count(newDate, wt=Probability) %>%summarise(mean=mean(n), sd=sd(n))
monthlySummedLong<- longData %>% group_by(tweetYear,tweetMonth) %>% count(Topic, wt=Probability) %>%
mutate(Topic = case_when(Topic == 0 ~ "Health Ch",
Topic == 1 ~ "Need 2 Qt",
Topic == 4 ~ "E-Cig",
Topic == 6 ~ "Clinics/Serv",
Topic == 7 ~ "Advice/Suc",
Topic == 8 ~ "Personal Ex")) %>% #note: other topics are NA unless we assign them something
mutate(tweetMonth = as.factor(tweetMonth)) %>%
ungroup() %>%
mutate(tweetMonth = as.factor(tweetMonth)) %>%
mutate(Quarter = case_when(tweetMonth %in% c(1,2,3) ~ "Q1",
tweetMonth %in% c(4,5,6) ~ "Q2",
tweetMonth %in% c(7,8,9) ~ "Q3",
tweetMonth %in% c(10,11,12) ~ "Q4"))
################################################################################
#Updated the data for new analyses                                             #
################################################################################
#LDA analysis in R
setwd("C:/Users/matth/Documents/GitHub/QS2020/QuitSmokingTweets")
library(tidyverse)
library(lubridate)
library(gridExtra)
library(lsmeans)
dat2018<- read.csv("C:/Users/matth/Documents/GitHub/QS2020/QuitSmokingTweets/processedWide2018.csv")
dat2019<- read.csv("C:/Users/matth/Documents/GitHub/QS2020/QuitSmokingTweets/processedWide2019.csv")
dat2020<- read.csv("C:/Users/matth/Documents/GitHub/QS2020/QuitSmokingTweets/processedWide2020.csv")
datNEW<- read.csv("C:/Users/matth/Documents/GitHub/QS2020/QuitSmokingTweets/processedWideNEW.csv")
wideData<- rbind(dat2018, dat2019, dat2020, datNEW) %>% mutate(newDate=as.Date(PUBLISH_DATE, "%m/%d/%Y")) %>%
mutate(tweetYear= year(newDate), tweetMonth= month(newDate), tweetDay=day(newDate)) %>%
group_by(tweetYear) %>% add_count(tweetYear, name="totalYearlyTweets") %>%
group_by(tweetMonth) %>% add_count(tweetMonth, name= "totalMonthlyTweets") %>%
ungroup() %>%
mutate(tweetWeek= week(newDate)) %>% add_count(tweetWeek, name= "totalWeeklyTweets") %>% #isoweek instead of week is probably needed for Trends data
distinct(ARTICLE_URL, .keep_all = TRUE) #when we brought in the new data there was overlap but we didnt remove it in the code because we needed identical dictionaries
#Separate out the topics and percentages, along with the date for graphing and the unique ID variable (ARTICLE_URL)
A <- wideData %>% select(ARTICLE_URL, CONTENT, newDate, tweetYear, tweetMonth, tweetWeek, zero_top, zero_perc) %>% rename(Topic = zero_top, Probability = zero_perc)
B <- wideData %>% select(ARTICLE_URL, CONTENT, newDate, tweetYear, tweetMonth, tweetWeek, one_top, one_perc) %>% filter(is.na(one_top)==FALSE) %>% rename(Topic = one_top, Probability = one_perc)
C <- wideData %>% select(ARTICLE_URL, CONTENT, newDate, tweetYear, tweetMonth, tweetWeek, two_top, two_perc) %>% filter(is.na(two_top)==FALSE) %>% rename(Topic = two_top, Probability = two_perc)
D <- wideData %>% select(ARTICLE_URL, CONTENT, newDate, tweetYear, tweetMonth, tweetWeek, three_top, three_perc) %>% filter(is.na(three_top)==FALSE) %>% rename(Topic = three_top, Probability = three_perc)
E <- wideData %>% select(ARTICLE_URL, CONTENT, newDate, tweetYear, tweetMonth, tweetWeek, four_top, four_perc) %>% filter(is.na(four_top)==FALSE) %>% rename(Topic = four_top, Probability = four_perc)
G <- wideData %>% select(ARTICLE_URL, CONTENT, newDate, tweetYear, tweetMonth, tweetWeek, five_top, five_perc) %>% filter(is.na(five_top)==FALSE) %>% rename(Topic = five_top, Probability = five_perc)
H <- wideData %>% select(ARTICLE_URL, CONTENT, newDate, tweetYear, tweetMonth, tweetWeek, six_top, six_perc) %>% filter(is.na(six_top)==FALSE) %>% rename(Topic = six_top, Probability = six_perc)
I <- wideData %>% select(ARTICLE_URL, CONTENT, newDate, tweetYear, tweetMonth, tweetWeek, seven_top, seven_perc) %>% filter(is.na(seven_top)==FALSE) %>% rename(Topic = seven_top, Probability = seven_perc)
J <- wideData %>% select(ARTICLE_URL, CONTENT, newDate, tweetYear, tweetMonth, tweetWeek, eight_top, eight_perc) %>% filter(is.na(eight_top)==FALSE) %>% rename(Topic = eight_top, Probability = eight_perc)
K <- wideData %>% select(ARTICLE_URL, CONTENT, newDate, tweetYear, tweetMonth, tweetWeek, nine_top, nine_perc) %>% filter(is.na(nine_top)==FALSE) %>% rename(Topic = nine_top, Probability = nine_perc)
longData<- rbind(A,B,C,D,E,G,H,I,J,K)
#add coronavirus mentions
longData$corona <- grepl("coronavirus", longData$CONTENT, ignore.case = TRUE)
longData$covid <- grepl("covid", longData$CONTENT, ignore.case = TRUE)
longData$hasCovid <- ifelse(longData$corona == TRUE | longData$covid == TRUE, 1, 0)
###########################################################################################################
#Lee wanted yearly average daily tweets and standard deviations
leeData<- longData %>% mutate(Topic = case_when(Topic == 0 ~ "Health Ch",
Topic == 1 ~ "Need 2 Qt",
Topic == 4 ~ "E-Cig",
Topic == 6 ~ "Clinics/Serv",
Topic == 7 ~ "Advice/Suc",
Topic == 8 ~ "Personal Ex")) %>%
filter(is.na(Topic)==FALSE) %>% group_by(tweetYear) %>% count(newDate, wt=Probability) %>%summarise(mean=mean(n), sd=sd(n))
monthlySummedLong<- longData %>% group_by(tweetYear,tweetMonth) %>% count(Topic, wt=Probability) %>%
mutate(Topic = case_when(Topic == 0 ~ "Health Ch",
Topic == 1 ~ "Need 2 Qt",
Topic == 4 ~ "E-Cig",
Topic == 6 ~ "Clinics/Serv",
Topic == 7 ~ "Advice/Suc",
Topic == 8 ~ "Personal Ex")) %>% #note: other topics are NA unless we assign them something
mutate(tweetMonth = as.factor(tweetMonth)) %>%
ungroup() %>%
mutate(tweetMonth = as.factor(tweetMonth)) %>%
mutate(Quarter = case_when(tweetMonth %in% c(1,2,3) ~ "Q1",
tweetMonth %in% c(4,5,6) ~ "Q2",
tweetMonth %in% c(7,8,9) ~ "Q3",
tweetMonth %in% c(10,11,12) ~ "Q4"))
CVmonthlySummedLong<- longData %>% filter(hasCovid==1) %>% group_by(tweetYear,tweetMonth) %>% count(Topic, wt=Probability) %>%
mutate(Topic = case_when(Topic == 0 ~ "Health Ch",
Topic == 1 ~ "Need 2 Qt",
Topic == 4 ~ "E-Cig",
Topic == 6 ~ "Clinics/Serv",
Topic == 7 ~ "Advice/Suc",
Topic == 8 ~ "Personal Ex")) %>% #note: other topics are NA unless we assign them something
mutate(tweetMonth = as.factor(tweetMonth)) %>%
ungroup() %>%
mutate(tweetMonth = as.factor(tweetMonth)) %>%
mutate(Quarter = case_when(tweetMonth %in% c(1,2,3) ~ "Q1",
tweetMonth %in% c(4,5,6) ~ "Q2",
tweetMonth %in% c(7,8,9) ~ "Q3",
tweetMonth %in% c(10,11,12) ~ "Q4"))
#######################################################################################################################
#For most of the tables
#######################################################################################################################
dailySummedLong<- longData %>% group_by(tweetYear,tweetMonth,newDate) %>% count(Topic, wt=Probability) %>%
mutate(Topic = case_when(Topic == 0 ~ "Health Ch",
Topic == 1 ~ "Need 2 Qt",
Topic == 4 ~ "E-Cig",
Topic == 6 ~ "Clinics/Serv",
Topic == 7 ~ "Advice/Suc",
Topic == 8 ~ "Personal Ex")) %>% #note: other topics are NA unless we assign them something
ungroup() %>%
mutate(tweetMonth = as.factor(tweetMonth)) %>%
mutate(Quarter = case_when(tweetMonth %in% c(1,2,3) ~ "Q1",
tweetMonth %in% c(4,5,6) ~ "Q2",
tweetMonth %in% c(7,8,9) ~ "Q3",
tweetMonth %in% c(10,11,12) ~ "Q4"))
#for coronavirus table
QuarterlyNoSpamTotal2020<- monthlySummedLong %>% filter(is.na(Topic)==FALSE, tweetYear==2020) %>%
group_by(tweetYear) %>% count(Quarter, wt=n)
QuarterlyNoSpamTotalTopics2020<- monthlySummedLong %>% filter(is.na(Topic)==FALSE, tweetYear==2020) %>%
group_by(tweetYear,Topic) %>% count(Quarter, wt=n)
#same as above but only corona
QuarterlyNoSpamTotal2020CV<- CVmonthlySummedLong %>% filter(is.na(Topic)==FALSE, tweetYear==2020) %>%
group_by(tweetYear) %>% count(Quarter, wt=n)
QuarterlyNoSpamTotalTopics2020CV<- CVmonthlySummedLong %>% filter(is.na(Topic)==FALSE, tweetYear==2020) %>%
group_by(tweetYear,Topic) %>% count(Quarter, wt=n)
################################################
dailyNoSpamSummedLong<- dailySummedLong %>% filter(is.na(Topic)==FALSE)
View(dailyNoSpamSummedLong)
View(QuarterlyNoSpamTotal2020)
View(dailyNoSpamSummedLong)
View(QuarterlyNoSpamTotal2020)
View(dailyNoSpamSummedLong)
View(dailyNoSpamSummedLong)
SupTable <- dailyNoSpamSummedLong %>% group_by(tweetYear) %>% count(Quarter, wt=n)
View(SupTable)
SupTable <- dailyNoSpamSummedLong %>% group_by(tweetYear, Topic) %>% count(Quarter, wt=n)
View(SupTable)
